{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STRAP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9vTDpDGME0P",
        "outputId": "ed2a6b94-0059-4ac8-95ae-b260c00877ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "COLAB_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "REPO_PATH = \"/content/drive/MyDrive/style-transfer-paraphrase\"\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHHitYLuOby-"
      },
      "source": [
        "Execute the readme setup from the [repo](https://github.com/martiansideofthemoon/style-transfer-paraphrase)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efxUh_qdNQL7",
        "outputId": "b9f65950-9a9c-467b-edd6-d2cc213f503d"
      },
      "source": [
        "# %cd \"/content/drive/MyDrive\"\n",
        "# !git clone https://github.com/ktxlh/style-transfer-paraphrase.git\n",
        "\n",
        "%cd $REPO_PATH\n",
        "\n",
        "!pip install torch torchvision\n",
        "!pip install -r requirements.txt\n",
        "!pip install --editable .\n",
        "\n",
        "!cd fairseq\n",
        "!pip install --editable .\n",
        "\n",
        "!pip install fairseq bitarray # workaround: to resolve some Colab specific module import error (https://github.com/pytorch/fairseq/issues/3093)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/style-transfer-paraphrase\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (8.0.0)\n",
            "Collecting en-core-web-sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
            "  Using cached en_core_web_sm-2.3.1-py3-none-any.whl\n",
            "Requirement already satisfied: blessings==1.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.7)\n",
            "Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: certifi==2020.6.20 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2020.6.20)\n",
            "Requirement already satisfied: cffi==1.14.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.14.3)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: cymem==2.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.0.4)\n",
            "Requirement already satisfied: Cython==0.29.21 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.29.21)\n",
            "Requirement already satisfied: dataclasses==0.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.6)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (3.0.12)\n",
            "Requirement already satisfied: flake8==3.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.8.4)\n",
            "Requirement already satisfied: Flask==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.1.2)\n",
            "Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (0.18.2)\n",
            "Requirement already satisfied: gpustat==0.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (0.6.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.10)\n",
            "Requirement already satisfied: importlib-metadata==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (2.0.0)\n",
            "Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.1.0)\n",
            "Requirement already satisfied: Jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (2.11.3)\n",
            "Requirement already satisfied: joblib==0.17.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (0.17.0)\n",
            "Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.1.1)\n",
            "Requirement already satisfied: mccabe==0.6.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (0.6.1)\n",
            "Requirement already satisfied: more-itertools==8.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (8.6.0)\n",
            "Requirement already satisfied: murmurhash==1.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (1.0.4)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 27)) (3.5)\n",
            "Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (1.19.2)\n",
            "Requirement already satisfied: nvidia-ml-py3==7.352.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (7.352.0)\n",
            "Requirement already satisfied: ordered-set==3.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 30)) (3.1.1)\n",
            "Requirement already satisfied: ordered-set-stubs==0.1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 31)) (0.1.3)\n",
            "Requirement already satisfied: packaging==20.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 32)) (20.4)\n",
            "Requirement already satisfied: Paste==3.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 33)) (3.5.0)\n",
            "Requirement already satisfied: pathtools==0.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 34)) (0.1.2)\n",
            "Requirement already satisfied: Pillow==8.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (8.0.0)\n",
            "Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 36)) (1.1.3)\n",
            "Requirement already satisfied: poetry-version==0.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (0.1.5)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (2.0.0)\n",
            "Requirement already satisfied: preshed==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 39)) (3.0.4)\n",
            "Requirement already satisfied: profanity-filter==1.3.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 40)) (1.3.3)\n",
            "Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 41)) (3.13.0)\n",
            "Requirement already satisfied: psutil==5.7.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 42)) (5.7.3)\n",
            "Requirement already satisfied: pycodestyle==2.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 43)) (2.6.0)\n",
            "Requirement already satisfied: pycparser==2.20 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (2.20)\n",
            "Requirement already satisfied: pydantic==1.7.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 45)) (1.7.4)\n",
            "Requirement already satisfied: pyflakes==2.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 46)) (2.2.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 47)) (2.4.7)\n",
            "Requirement already satisfied: redis==3.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 48)) (3.5.3)\n",
            "Requirement already satisfied: regex==2020.10.15 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 49)) (2020.10.15)\n",
            "Requirement already satisfied: requests==2.24.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 50)) (2.24.0)\n",
            "Requirement already satisfied: ruamel.yaml==0.15.100 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 51)) (0.15.100)\n",
            "Requirement already satisfied: sacrebleu==1.4.14 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 52)) (1.4.14)\n",
            "Requirement already satisfied: sacremoses==0.0.43 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 53)) (0.0.43)\n",
            "Requirement already satisfied: scipy==1.5.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 54)) (1.5.4)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 55)) (0.1.91)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 56)) (1.15.0)\n",
            "Requirement already satisfied: spacy==2.3.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 57)) (2.3.2)\n",
            "Requirement already satisfied: srsly==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 58)) (1.0.3)\n",
            "Requirement already satisfied: tensorboardX==2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 59)) (2.1)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 60)) (7.4.1)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 61)) (0.9.2)\n",
            "Requirement already satisfied: tomlkit==0.5.11 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 62)) (0.5.11)\n",
            "Requirement already satisfied: tqdm==4.50.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 63)) (4.50.2)\n",
            "Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 64)) (3.4.0)\n",
            "Requirement already satisfied: urllib3==1.25.11 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 65)) (1.25.11)\n",
            "Requirement already satisfied: waitress==1.4.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 66)) (1.4.4)\n",
            "Requirement already satisfied: wasabi==0.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 67)) (0.8.0)\n",
            "Requirement already satisfied: watchdog==0.10.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 68)) (0.10.3)\n",
            "Requirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 69)) (1.0.1)\n",
            "Requirement already satisfied: zipp==3.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 70)) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from Paste==3.5.0->-r requirements.txt (line 33)) (57.4.0)\n",
            "Obtaining file:///content/drive/My%20Drive/style-transfer-paraphrase\n",
            "Installing collected packages: style-paraphrase\n",
            "  Attempting uninstall: style-paraphrase\n",
            "    Found existing installation: style-paraphrase 1.0\n",
            "    Can't uninstall 'style-paraphrase'. No files were found to uninstall.\n",
            "  Running setup.py develop for style-paraphrase\n",
            "Successfully installed style-paraphrase-1.0\n",
            "Obtaining file:///content/drive/My%20Drive/style-transfer-paraphrase\n",
            "Installing collected packages: style-paraphrase\n",
            "  Attempting uninstall: style-paraphrase\n",
            "    Found existing installation: style-paraphrase 1.0\n",
            "    Can't uninstall 'style-paraphrase'. No files were found to uninstall.\n",
            "  Running setup.py develop for style-paraphrase\n",
            "Successfully installed style-paraphrase-1.0\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.7/dist-packages (2.3.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.21)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2020.10.15)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.4.14)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.50.2)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.10.0+cu111)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (2.1.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (6.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core->fairseq) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0IkDORP9AB"
      },
      "source": [
        "**`Restart runtime` now for the first time you run this notebook in each session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLvSSNIlNQq3",
        "outputId": "3533b83e-1cc1-4d0a-e24d-ba0ecccbe844"
      },
      "source": [
        "%cd /content/\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n",
        "!tar -xzvf roberta.large.tar.gz\n",
        "\n",
        "# Add the following to your .bashrc file, feel free to store the model elsewhere on the hard disk\n",
        "!export ROBERTA_LARGE=$PWD/roberta.large\n",
        "%cd $REPO_PATH"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2021-12-04 02:28:14--  https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 655283069 (625M) [application/gzip]\n",
            "Saving to: ‘roberta.large.tar.gz’\n",
            "\n",
            "roberta.large.tar.g 100%[===================>] 624.93M  12.0MB/s    in 54s     \n",
            "\n",
            "2021-12-04 02:29:09 (11.6 MB/s) - ‘roberta.large.tar.gz’ saved [655283069/655283069]\n",
            "\n",
            "roberta.large/\n",
            "roberta.large/dict.txt\n",
            "roberta.large/model.pt\n",
            "roberta.large/NOTE\n",
            "/content/drive/MyDrive/style-transfer-paraphrase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zswas7oTi26i"
      },
      "source": [
        "custom data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-x3DWylfTA7"
      },
      "source": [
        "aave_data_name, sae_data_name = \"aae\", \"tweets\"\n",
        "# !mkdir datasets/$aave_data_name datasets/$sae_data_name"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTwh9C_SiyBq"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "data_input_path = f\"{COLAB_PATH}/AAVE_SAE/data/training\"\n",
        "split_paths = {x.split('_')[0]: os.path.join(data_input_path, x) for x in os.listdir(data_input_path)}\n",
        "\n",
        "split_name_map = {\n",
        "    'train': 'train',\n",
        "    'val': 'dev',\n",
        "    'test': 'test',\n",
        "}\n",
        "def conver_to_strap_data(split_name, split_path):\n",
        "    df = pd.read_csv(split_path)\n",
        "    aave_x = df.sentence[df.label==1].tolist()\n",
        "    sae_x = df.sentence[df.label==0].tolist()\n",
        "    with open(os.path.join('datasets', aave_data_name, split_name_map[split_name] + '.txt'), 'w') as f:\n",
        "        f.write('\\n'.join(aave_x) + '\\n')\n",
        "    with open(os.path.join('datasets', sae_data_name, split_name_map[split_name] + '.txt'), 'w') as f:\n",
        "        f.write('\\n'.join(sae_x) + '\\n')\n",
        "    with open(os.path.join('datasets', aave_data_name, split_name_map[split_name] + '.label'), 'w') as f:\n",
        "        f.write(''.join([aave_data_name + '\\n' for _ in range(len(aave_x))]))\n",
        "    with open(os.path.join('datasets', sae_data_name, split_name_map[split_name] + '.label'), 'w') as f:\n",
        "        f.write(''.join([sae_data_name + '\\n' for _ in range(len(sae_x))]))\n",
        "\n",
        "for k, v in split_paths.items():\n",
        "    conver_to_strap_data(k, v)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB6ZBz0a3Vm-"
      },
      "source": [
        "BPE conversion: steps from readme.md\n",
        "\n",
        "1. To convert a plaintext dataset into it's BPE form run the command,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjXON74o29vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abb10cd-8e1a-4189-92b4-7b8cdc02206b"
      },
      "source": [
        "!python datasets/dataset2bpe.py --dataset datasets/$aave_data_name\n",
        "!python datasets/dataset2bpe.py --dataset datasets/$sae_data_name"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "running build_ext\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:381: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "skipping 'fairseq/data/data_utils_fast.cpp' Cython extension (up-to-date)\n",
            "skipping 'fairseq/data/token_block_utils_fast.cpp' Cython extension (up-to-date)\n",
            "building 'alignment_train_cpu_binding' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c examples/operators/alignment_train_cpu.cpp -o build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=alignment_train_cpu_binding -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/alignment_train_cpu_binding.cpython-37m-x86_64-linux-gnu.so\n",
            "2021-12-04 02:31:58 | INFO | root | Generating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "2021-12-04 02:31:58 | INFO | root | Generating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libbase.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.7/alignment_train_cpu_binding.cpython-37m-x86_64-linux-gnu.so -> \n",
            "2021-12-04 02:31:59 | INFO | fairseq.file_utils | http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz not found in cache, downloading to /tmp/tmpaqn6g_nm\n",
            "100% 231160875/231160875 [00:18<00:00, 12214620.40B/s]\n",
            "2021-12-04 02:32:19 | INFO | fairseq.file_utils | copying /tmp/tmpaqn6g_nm to cache at /root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
            "2021-12-04 02:32:19 | INFO | fairseq.file_utils | creating metadata file for /root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
            "2021-12-04 02:32:19 | INFO | fairseq.file_utils | removing temp file /tmp/tmpaqn6g_nm\n",
            "2021-12-04 02:32:19 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at /root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
            "2021-12-04 02:32:19 | INFO | fairseq.file_utils | extracting archive file /root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350 to temp dir /tmp/tmpfc4qpw06\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  message=\"hydra.experimental.initialize() is no longer experimental.\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/compose.py:19: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
            "  message=\"hydra.experimental.compose() is no longer experimental.\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/core/default_element.py:126: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  See {url} for more information\"\"\"\n",
            "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:418: UserWarning: \n",
            "'config' is validated against ConfigStore schema with the same name.\n",
            "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
            "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/compose.py:54: UserWarning: \n",
            "The strict flag in the compose API is deprecated and will be removed in the next version of Hydra.\n",
            "See https://hydra.cc/docs/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
            "\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  message=\"hydra.experimental.initialize() is no longer experimental.\"\n",
            "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:285: UserWarning: \n",
            "'config' is validated against ConfigStore schema with the same name.\n",
            "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
            "  **kwargs,\n",
            "2021-12-04 02:32:25 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n",
            "2021-12-04 02:32:29 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 512, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19812, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 999999, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 999999, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 512}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='loss', bpe='gpt2', bucket_cap_mb=200, clip_norm=0.0, cpu=False, criterion='masked_lm', curriculum=0, data='/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', dataset_impl='mmap', ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_no_spawn=False, distributed_port=19812, distributed_rank=0, distributed_world_size=512, dropout=0.1, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, find_unused_parameters=True, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, global_sync_iter=10, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, log_format='json', log_interval=25, lr=[0.0006], lr_scheduler='polynomial_decay', mask_prob=0.15, max_epoch=0, max_positions=512, max_sentences=16, max_sentences_valid=16, max_source_positions=512, max_target_positions=512, max_tokens=999999, max_update=500000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=2, only_validate=False, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, random_token_prob=0.1, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_interval=1, save_interval_updates=2000, seed=1, sentence_avg=False, skip_invalid_size_inputs_valid_test=True, spectral_norm_classification_head=False, stop_min_lr=-1, task='masked_lm', tbmf_wrapper=False, threshold_loss_scale=1.0, tokenizer=None, tokens_per_sample=512, total_num_update=500000, train_subset='train', untie_weights_roberta=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=24000, weight_decay=0.01), 'task': {'_name': 'masked_lm', 'data': '/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2021-12-04 02:32:30 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json not found in cache, downloading to /tmp/tmp4b96w5dg\n",
            "1042301B [00:04, 248441.25B/s]\n",
            "2021-12-04 02:32:35 | INFO | fairseq.file_utils | copying /tmp/tmp4b96w5dg to cache at /root/.cache/torch/pytorch_fairseq/e2aab4d600e7568c2d88fc7732130ccc815ea84ec63906cb0913c7a3a4906a2e.0f323dfaed92d080380e63f0291d0f31adfa8c61a62cbcb3cb8114f061be27f7\n",
            "2021-12-04 02:32:35 | INFO | fairseq.file_utils | creating metadata file for /root/.cache/torch/pytorch_fairseq/e2aab4d600e7568c2d88fc7732130ccc815ea84ec63906cb0913c7a3a4906a2e.0f323dfaed92d080380e63f0291d0f31adfa8c61a62cbcb3cb8114f061be27f7\n",
            "2021-12-04 02:32:35 | INFO | fairseq.file_utils | removing temp file /tmp/tmp4b96w5dg\n",
            "2021-12-04 02:32:36 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe not found in cache, downloading to /tmp/tmp5wusbbz0\n",
            "456318B [00:00, 695848.84B/s]\n",
            "2021-12-04 02:32:37 | INFO | fairseq.file_utils | copying /tmp/tmp5wusbbz0 to cache at /root/.cache/torch/pytorch_fairseq/b04a6d337c09f464fe8f0df1d3524db88a597007d63f05d97e437f65840cdba5.939bed25cbdab15712bac084ee713d6c78e221c5156c68cb0076b03f5170600f\n",
            "2021-12-04 02:32:37 | INFO | fairseq.file_utils | creating metadata file for /root/.cache/torch/pytorch_fairseq/b04a6d337c09f464fe8f0df1d3524db88a597007d63f05d97e437f65840cdba5.939bed25cbdab15712bac084ee713d6c78e221c5156c68cb0076b03f5170600f\n",
            "2021-12-04 02:32:37 | INFO | fairseq.file_utils | removing temp file /tmp/tmp5wusbbz0\n",
            "100% 1615/1615 [00:00<00:00, 3800.08it/s]\n",
            "100% 202/202 [00:00<00:00, 4734.56it/s]\n",
            "100% 202/202 [00:00<00:00, 5336.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg3sYNL49lxV"
      },
      "source": [
        "2. Next, for converting the BPE codes to fairseq binaries and building a label dictionary, first make sure you have downloaded RoBERTa and setup the $ROBERTA_LARGE global variable in your .bashrc (see \"Setup\" for more details). Then run,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow9MDjBh8c3r",
        "outputId": "69493364-7594-44de-a7e1-524c4936e74d"
      },
      "source": [
        "!chmod +x datasets/bpe2binary.sh\n",
        "!cp \"/content/roberta.large/dict.txt\" /dict.txt\n",
        "# !datasets/bpe2binary.sh datasets/$aave_data_name\n",
        "!datasets/bpe2binary.sh datasets/$sae_data_name"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-04 02:32:43 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='datasets/tweets-bin/label', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang=None, srcdict=None, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='datasets/tweets/train.label', user_dir=None, validpref='datasets/tweets/dev.label', workers=24)\n",
            "2021-12-04 02:32:44 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2021-12-04 02:32:44 | INFO | fairseq_cli.preprocess | [None] datasets/tweets/train.label: 1615 sents, 3230 tokens, 0.0% replaced by <unk>\n",
            "2021-12-04 02:32:44 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2021-12-04 02:32:45 | INFO | fairseq_cli.preprocess | [None] datasets/tweets/dev.label: 202 sents, 404 tokens, 0.0% replaced by <unk>\n",
            "2021-12-04 02:32:45 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to datasets/tweets-bin/label\n",
            "2021-12-04 02:32:48 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='datasets/tweets-bin/input0', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang=None, srcdict='/dict.txt', target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='datasets/tweets/train.input0.bpe', user_dir=None, validpref='datasets/tweets/dev.input0.bpe', workers=24)\n",
            "2021-12-04 02:32:48 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2021-12-04 02:32:49 | INFO | fairseq_cli.preprocess | [None] datasets/tweets/train.input0.bpe: 1615 sents, 46383 tokens, 0.0% replaced by <unk>\n",
            "2021-12-04 02:32:49 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2021-12-04 02:32:51 | INFO | fairseq_cli.preprocess | [None] datasets/tweets/dev.input0.bpe: 202 sents, 5501 tokens, 0.0% replaced by <unk>\n",
            "2021-12-04 02:32:51 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to datasets/tweets-bin/input0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJLn40h3-2wl"
      },
      "source": [
        "3. To train inverse paraphrasers you will need to paraphrase the dataset. First, download the pretrained model paraphraser_gpt2_large from here. After downloading the pretrained paraphrase model run the command,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8rNvxmW_Oi3",
        "outputId": "23853188-b368-4504-ca29-739c2f115942"
      },
      "source": [
        "# !python datasets/paraphrase_splits.py --dataset datasets/$aave_data_name\n",
        "!python datasets/paraphrase_splits.py --dataset datasets/$sae_data_name"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
            "2021-12-04 02:33:03 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at /root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  message=\"hydra.experimental.initialize() is no longer experimental.\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/compose.py:19: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
            "  message=\"hydra.experimental.compose() is no longer experimental.\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/core/default_element.py:126: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  See {url} for more information\"\"\"\n",
            "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:418: UserWarning: \n",
            "'config' is validated against ConfigStore schema with the same name.\n",
            "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
            "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/compose.py:54: UserWarning: \n",
            "The strict flag in the compose API is deprecated and will be removed in the next version of Hydra.\n",
            "See https://hydra.cc/docs/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
            "\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  message=\"hydra.experimental.initialize() is no longer experimental.\"\n",
            "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:285: UserWarning: \n",
            "'config' is validated against ConfigStore schema with the same name.\n",
            "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
            "  **kwargs,\n",
            "2021-12-04 02:33:05 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n",
            "2021-12-04 02:33:09 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 512, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19812, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 999999, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 999999, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 512}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='loss', bpe='gpt2', bucket_cap_mb=200, clip_norm=0.0, cpu=False, criterion='masked_lm', curriculum=0, data='/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', dataset_impl='mmap', ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_no_spawn=False, distributed_port=19812, distributed_rank=0, distributed_world_size=512, dropout=0.1, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, find_unused_parameters=True, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, global_sync_iter=10, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, log_format='json', log_interval=25, lr=[0.0006], lr_scheduler='polynomial_decay', mask_prob=0.15, max_epoch=0, max_positions=512, max_sentences=16, max_sentences_valid=16, max_source_positions=512, max_target_positions=512, max_tokens=999999, max_update=500000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=2, only_validate=False, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, random_token_prob=0.1, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_interval=1, save_interval_updates=2000, seed=1, sentence_avg=False, skip_invalid_size_inputs_valid_test=True, spectral_norm_classification_head=False, stop_min_lr=-1, task='masked_lm', tbmf_wrapper=False, threshold_loss_scale=1.0, tokenizer=None, tokens_per_sample=512, total_num_update=500000, train_subset='train', untie_weights_roberta=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=24000, weight_decay=0.01), 'task': {'_name': 'masked_lm', 'data': '/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "Loading paraphraser...\n",
            "Some weights of the model checkpoint at paraphraser_gpt2_large were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "minibatches of train split done...:   0% 0/26 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n",
            "minibatches of train split done...: 100% 26/26 [01:54<00:00,  4.39s/it]\n",
            "minibatches of dev split done...: 100% 4/4 [00:15<00:00,  3.88s/it]\n",
            "minibatches of test split done...: 100% 4/4 [00:15<00:00,  3.87s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYDGyb8ZDq70"
      },
      "source": [
        "Step 4 and 5 are for training. We don't need them.\n",
        "> 4. Add an entry to the DATASET_CONFIG dictionary in style_paraphrase/dataset_config.py, customizing configuration if needed.\n",
        "> 5. Enter your dataset in the hyperparameters file (style_paraphrase/hyperparameters_config.py) and run python style_paraphrase/schedule.py.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI1oKxNnHTKu",
        "outputId": "0073f484-56f9-4402-d718-76d39eacd7ed"
      },
      "source": [
        "!chmod +x \"$REPO_PATH/style_paraphrase/evaluation/scripts/evaluate_twitter.sh\"\n",
        "!style_paraphrase/evaluation/scripts/evaluate_twitter.sh \\\n",
        "    \"$REPO_PATH/aae\" \"$REPO_PATH/tweets\" \"$REPO_PATH/paraphraser_gpt2_large\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------------------------\n",
            "Mode nucleus_paraphrase  --- top-p 0.0, split test\n",
            "-------------------------------------------\n",
            "\n",
            "\n",
            "translate tweets to aae\n",
            "Output already exists...\n",
            "\n",
            "translate aae to tweets\n",
            "Output already exists...\n",
            "\n",
            "RoBERTa test classification\n",
            "\n",
            "tcmalloc: large alloc 1425891328 bytes == 0x55ea8ea8e000 @  0x7fa335aa7b6b 0x7fa335ac7379 0x7fa21f2bdcde 0x7fa21f2bf452 0x7fa271d31749 0x7fa31ab5ad59 0x55ea5e277045 0x55ea5e237c52 0x55ea5e2aac25 0x55ea5e2a5ced 0x55ea5e2387f3 0x55ea5e2382f9 0x55ea5e37f35d 0x55ea5e2eea0b 0x55ea5e2373a1 0x55ea5e328e1d 0x55ea5e2aae99 0x55ea5e2a5ced 0x55ea5e177e2b 0x55ea5e2a7fe4 0x55ea5e2a59ee 0x55ea5e238bda 0x55ea5e2a7737 0x55ea5e2a59ee 0x55ea5e238bda 0x55ea5e2a6915 0x55ea5e2a59ee 0x55ea5e238bda 0x55ea5e2a7737 0x55ea5e2a5ced 0x55ea5e177e2b\n",
            "tcmalloc: large alloc 1425891328 bytes == 0x55eae3a64000 @  0x7fa335aa7b6b 0x7fa335ac7379 0x7fa21f2bdcde 0x7fa21f2bf452 0x7fa271d31749 0x7fa31ab5ad59 0x55ea5e277045 0x55ea5e237c52 0x55ea5e2aac25 0x55ea5e2a5ced 0x55ea5e2387f3 0x55ea5e2382f9 0x55ea5e37f35d 0x55ea5e2eea0b 0x55ea5e2373a1 0x55ea5e328e1d 0x55ea5e2aae99 0x55ea5e2a5ced 0x55ea5e177e2b 0x55ea5e2a7fe4 0x55ea5e2a59ee 0x55ea5e238bda 0x55ea5e2a7737 0x55ea5e2a59ee 0x55ea5e238bda 0x55ea5e2a6915 0x55ea5e2a59ee 0x55ea5e238bda 0x55ea5e2a7737 0x55ea5e2a5ced 0x55ea5e177e2b\n",
            "26it [00:01, 16.29it/s]            \n",
            "\n",
            "author \u001b[1menglish_tweet           \u001b[0m = \u001b[1m\u001b[92m 67.82\u001b[0m (137 / 202)\n",
            "author \u001b[1maae                     \u001b[0m = \u001b[1m\u001b[92m 70.79\u001b[0m (143 / 202)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\u001b[1moverall accuracy               \u001b[0m = \u001b[1m\u001b[92m 69.31\u001b[0m (280 / 404)\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "\n",
            "RoBERTa acceptability classification\n",
            "\n",
            "tcmalloc: large alloc 1425858560 bytes == 0x55c786ec0000 @  0x7fa939939b6b 0x7fa939959379 0x7fa82314fcde 0x7fa823151452 0x7fa875bc3749 0x7fa91e9ecd59 0x55c755879045 0x55c755839c52 0x55c7558acc25 0x55c7558a7ced 0x55c75583a7f3 0x55c75583a2f9 0x55c75598135d 0x55c7558f0a0b 0x55c7558393a1 0x55c75592ae1d 0x55c7558ace99 0x55c7558a7ced 0x55c755779e2b 0x55c7558a9fe4 0x55c7558a79ee 0x55c75583abda 0x55c7558a9737 0x55c7558a79ee 0x55c75583abda 0x55c7558a8915 0x55c7558a79ee 0x55c75583abda 0x55c7558a9737 0x55c7558a7ced 0x55c755779e2b\n",
            "tcmalloc: large alloc 1425858560 bytes == 0x55c7dbe8e000 @  0x7fa939939b6b 0x7fa939959379 0x7fa82314fcde 0x7fa823151452 0x7fa875bc3749 0x7fa91e9ecd59 0x55c755879045 0x55c755839c52 0x55c7558acc25 0x55c7558a7ced 0x55c75583a7f3 0x55c75583a2f9 0x55c75598135d 0x55c7558f0a0b 0x55c7558393a1 0x55c75592ae1d 0x55c7558ace99 0x55c7558a7ced 0x55c755779e2b 0x55c7558a9fe4 0x55c7558a79ee 0x55c75583abda 0x55c7558a9737 0x55c7558a79ee 0x55c75583abda 0x55c7558a8915 0x55c7558a79ee 0x55c75583abda 0x55c7558a9737 0x55c7558a7ced 0x55c755779e2b\n",
            "26it [00:01, 16.28it/s]            \n",
            "\n",
            "author \u001b[1macceptable              \u001b[0m = \u001b[1m\u001b[92m 82.67\u001b[0m (334 / 404)\n",
            "author \u001b[1munacceptable            \u001b[0m = \u001b[1m\u001b[92m  0.00\u001b[0m (  0 /   1)\n",
            "────────────────────────────────────────────────────────────\n",
            "\u001b[1moverall accuracy               \u001b[0m = \u001b[1m\u001b[92m 82.67\u001b[0m (334 / 404)\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "\n",
            "Paraphrase scores --- generated vs inputs..\n",
            "\n",
            "100% 26/26 [00:00<00:00, 141.67it/s]\n",
            "Avg similarity score vs inputs = 0.654411\n",
            "Avg max score = 0.654411\n",
            "\n",
            "Paraphrase scores --- generated vs gold..\n",
            "\n",
            "100% 26/26 [00:00<00:00, 160.24it/s]\n",
            "Avg similarity score vs gold = 0.567724\n",
            "Avg max score = 0.567724\n",
            "\n",
            " final normalized scores vs gold..\n",
            "\n",
            "Normalized pp score (acc_sim) = 0.3945 (280 / 404 valid)\n",
            "Normalized pp score (cola_sim) = 0.4715 (334 / 404 valid)\n",
            "Normalized pp score (acc_cola) = 0.5619 (227 / 404 valid)\n",
            "Normalized pp score (acc_cola_sim) = 0.3227 (227 / 404 valid)\n",
            "\n",
            "-------------------------------------------\n",
            "Mode nucleus_paraphrase  --- top-p 0.6, split test\n",
            "-------------------------------------------\n",
            "\n",
            "\n",
            "translate tweets to aae\n",
            "Output already exists...\n",
            "\n",
            "translate aae to tweets\n",
            "Output already exists...\n",
            "\n",
            "RoBERTa test classification\n",
            "\n",
            "tcmalloc: large alloc 1425891328 bytes == 0x55eb78a8c000 @  0x7f13193e8b6b 0x7f1319408379 0x7f1202bfecde 0x7f1202c00452 0x7f1255672749 0x7f12fe49bd59 0x55eb482d3045 0x55eb48293c52 0x55eb48306c25 0x55eb48301ced 0x55eb482947f3 0x55eb482942f9 0x55eb483db35d 0x55eb4834aa0b 0x55eb482933a1 0x55eb48384e1d 0x55eb48306e99 0x55eb48301ced 0x55eb481d3e2b 0x55eb48303fe4 0x55eb483019ee 0x55eb48294bda 0x55eb48303737 0x55eb483019ee 0x55eb48294bda 0x55eb48302915 0x55eb483019ee 0x55eb48294bda 0x55eb48303737 0x55eb48301ced 0x55eb481d3e2b\n",
            "tcmalloc: large alloc 1425891328 bytes == 0x55ebcda62000 @  0x7f13193e8b6b 0x7f1319408379 0x7f1202bfecde 0x7f1202c00452 0x7f1255672749 0x7f12fe49bd59 0x55eb482d3045 0x55eb48293c52 0x55eb48306c25 0x55eb48301ced 0x55eb482947f3 0x55eb482942f9 0x55eb483db35d 0x55eb4834aa0b 0x55eb482933a1 0x55eb48384e1d 0x55eb48306e99 0x55eb48301ced 0x55eb481d3e2b 0x55eb48303fe4 0x55eb483019ee 0x55eb48294bda 0x55eb48303737 0x55eb483019ee 0x55eb48294bda 0x55eb48302915 0x55eb483019ee 0x55eb48294bda 0x55eb48303737 0x55eb48301ced 0x55eb481d3e2b\n",
            "26it [00:01, 16.41it/s]            \n",
            "\n",
            "author \u001b[1menglish_tweet           \u001b[0m = \u001b[1m\u001b[92m 66.34\u001b[0m (134 / 202)\n",
            "author \u001b[1maae                     \u001b[0m = \u001b[1m\u001b[92m 72.28\u001b[0m (146 / 202)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\u001b[1moverall accuracy               \u001b[0m = \u001b[1m\u001b[92m 69.31\u001b[0m (280 / 404)\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "\n",
            "RoBERTa acceptability classification\n",
            "\n",
            "tcmalloc: large alloc 1425858560 bytes == 0x559c05d60000 @  0x7f0c8c4bab6b 0x7f0c8c4da379 0x7f0b75cd0cde 0x7f0b75cd2452 0x7f0bc8744749 0x7f0c7156dd59 0x559bd4a3e045 0x559bd49fec52 0x559bd4a71c25 0x559bd4a6cced 0x559bd49ff7f3 0x559bd49ff2f9 0x559bd4b4635d 0x559bd4ab5a0b 0x559bd49fe3a1 0x559bd4aefe1d 0x559bd4a71e99 0x559bd4a6cced 0x559bd493ee2b 0x559bd4a6efe4 0x559bd4a6c9ee 0x559bd49ffbda 0x559bd4a6e737 0x559bd4a6c9ee 0x559bd49ffbda 0x559bd4a6d915 0x559bd4a6c9ee 0x559bd49ffbda 0x559bd4a6e737 0x559bd4a6cced 0x559bd493ee2b\n",
            "tcmalloc: large alloc 1425858560 bytes == 0x559c5ad2e000 @  0x7f0c8c4bab6b 0x7f0c8c4da379 0x7f0b75cd0cde 0x7f0b75cd2452 0x7f0bc8744749 0x7f0c7156dd59 0x559bd4a3e045 0x559bd49fec52 0x559bd4a71c25 0x559bd4a6cced 0x559bd49ff7f3 0x559bd49ff2f9 0x559bd4b4635d 0x559bd4ab5a0b 0x559bd49fe3a1 0x559bd4aefe1d 0x559bd4a71e99 0x559bd4a6cced 0x559bd493ee2b 0x559bd4a6efe4 0x559bd4a6c9ee 0x559bd49ffbda 0x559bd4a6e737 0x559bd4a6c9ee 0x559bd49ffbda 0x559bd4a6d915 0x559bd4a6c9ee 0x559bd49ffbda 0x559bd4a6e737 0x559bd4a6cced 0x559bd493ee2b\n",
            "26it [00:01, 16.51it/s]            \n",
            "\n",
            "author \u001b[1macceptable              \u001b[0m = \u001b[1m\u001b[92m 75.74\u001b[0m (306 / 404)\n",
            "author \u001b[1munacceptable            \u001b[0m = \u001b[1m\u001b[92m  0.00\u001b[0m (  0 /   1)\n",
            "────────────────────────────────────────────────────────────\n",
            "\u001b[1moverall accuracy               \u001b[0m = \u001b[1m\u001b[92m 75.74\u001b[0m (306 / 404)\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "\n",
            "Paraphrase scores --- generated vs inputs..\n",
            "\n",
            "100% 26/26 [00:00<00:00, 181.38it/s]\n",
            "Avg similarity score vs inputs = 0.623008\n",
            "Avg max score = 0.623008\n",
            "\n",
            "Paraphrase scores --- generated vs gold..\n",
            "\n",
            "100% 26/26 [00:00<00:00, 180.97it/s]\n",
            "Avg similarity score vs gold = 0.542139\n",
            "Avg max score = 0.542139\n",
            "\n",
            " final normalized scores vs gold..\n",
            "\n",
            "Normalized pp score (acc_sim) = 0.3808 (280 / 404 valid)\n",
            "Normalized pp score (cola_sim) = 0.4115 (306 / 404 valid)\n",
            "Normalized pp score (acc_cola) = 0.5272 (213 / 404 valid)\n",
            "Normalized pp score (acc_cola_sim) = 0.2893 (213 / 404 valid)\n",
            "\n",
            "-------------------------------------------\n",
            "Mode nucleus_paraphrase  --- top-p 0.9, split test\n",
            "-------------------------------------------\n",
            "\n",
            "\n",
            "translate tweets to aae\n",
            "Output already exists...\n",
            "\n",
            "translate aae to tweets\n",
            "Output already exists...\n",
            "\n",
            "RoBERTa test classification\n",
            "\n",
            "tcmalloc: large alloc 1425891328 bytes == 0x564c81042000 @  0x7fbd8ebfab6b 0x7fbd8ec1a379 0x7fbc78410cde 0x7fbc78412452 0x7fbccae84749 0x7fbd73cadd59 0x564c50482045 0x564c50442c52 0x564c504b5c25 0x564c504b0ced 0x564c504437f3 0x564c504432f9 0x564c5058a35d 0x564c504f9a0b 0x564c504423a1 0x564c50533e1d 0x564c504b5e99 0x564c504b0ced 0x564c50382e2b 0x564c504b2fe4 0x564c504b09ee 0x564c50443bda 0x564c504b2737 0x564c504b09ee 0x564c50443bda 0x564c504b1915 0x564c504b09ee 0x564c50443bda 0x564c504b2737 0x564c504b0ced 0x564c50382e2b\n",
            "tcmalloc: large alloc 1425891328 bytes == 0x564cd6018000 @  0x7fbd8ebfab6b 0x7fbd8ec1a379 0x7fbc78410cde 0x7fbc78412452 0x7fbccae84749 0x7fbd73cadd59 0x564c50482045 0x564c50442c52 0x564c504b5c25 0x564c504b0ced 0x564c504437f3 0x564c504432f9 0x564c5058a35d 0x564c504f9a0b 0x564c504423a1 0x564c50533e1d 0x564c504b5e99 0x564c504b0ced 0x564c50382e2b 0x564c504b2fe4 0x564c504b09ee 0x564c50443bda 0x564c504b2737 0x564c504b09ee 0x564c50443bda 0x564c504b1915 0x564c504b09ee 0x564c50443bda 0x564c504b2737 0x564c504b0ced 0x564c50382e2b\n",
            "26it [00:01, 15.19it/s]            \n",
            "\n",
            "author \u001b[1menglish_tweet           \u001b[0m = \u001b[1m\u001b[92m 69.31\u001b[0m (140 / 202)\n",
            "author \u001b[1maae                     \u001b[0m = \u001b[1m\u001b[92m 79.21\u001b[0m (160 / 202)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\u001b[1moverall accuracy               \u001b[0m = \u001b[1m\u001b[92m 74.26\u001b[0m (300 / 404)\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "\n",
            "RoBERTa acceptability classification\n",
            "\n",
            "tcmalloc: large alloc 1425858560 bytes == 0x562909bb0000 @  0x7f74936a8b6b 0x7f74936c8379 0x7f737cebecde 0x7f737cec0452 0x7f73cf932749 0x7f747875bd59 0x5628d9432045 0x5628d93f2c52 0x5628d9465c25 0x5628d9460ced 0x5628d93f37f3 0x5628d93f32f9 0x5628d953a35d 0x5628d94a9a0b 0x5628d93f23a1 0x5628d94e3e1d 0x5628d9465e99 0x5628d9460ced 0x5628d9332e2b 0x5628d9462fe4 0x5628d94609ee 0x5628d93f3bda 0x5628d9462737 0x5628d94609ee 0x5628d93f3bda 0x5628d9461915 0x5628d94609ee 0x5628d93f3bda 0x5628d9462737 0x5628d9460ced 0x5628d9332e2b\n",
            "tcmalloc: large alloc 1425858560 bytes == 0x56295eb7e000 @  0x7f74936a8b6b 0x7f74936c8379 0x7f737cebecde 0x7f737cec0452 0x7f73cf932749 0x7f747875bd59 0x5628d9432045 0x5628d93f2c52 0x5628d9465c25 0x5628d9460ced 0x5628d93f37f3 0x5628d93f32f9 0x5628d953a35d 0x5628d94a9a0b 0x5628d93f23a1 0x5628d94e3e1d 0x5628d9465e99 0x5628d9460ced 0x5628d9332e2b 0x5628d9462fe4 0x5628d94609ee 0x5628d93f3bda 0x5628d9462737 0x5628d94609ee 0x5628d93f3bda 0x5628d9461915 0x5628d94609ee 0x5628d93f3bda 0x5628d9462737 0x5628d9460ced 0x5628d9332e2b\n",
            "26it [00:01, 15.38it/s]            \n",
            "\n",
            "author \u001b[1macceptable              \u001b[0m = \u001b[1m\u001b[92m 56.44\u001b[0m (228 / 404)\n",
            "author \u001b[1munacceptable            \u001b[0m = \u001b[1m\u001b[92m  0.00\u001b[0m (  0 /   1)\n",
            "────────────────────────────────────────────────────────────\n",
            "\u001b[1moverall accuracy               \u001b[0m = \u001b[1m\u001b[92m 56.44\u001b[0m (228 / 404)\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "\n",
            "Paraphrase scores --- generated vs inputs..\n",
            "\n",
            "100% 26/26 [00:00<00:00, 162.08it/s]\n",
            "Avg similarity score vs inputs = 0.551173\n",
            "Avg max score = 0.551173\n",
            "\n",
            "Paraphrase scores --- generated vs gold..\n",
            "\n",
            "100% 26/26 [00:00<00:00, 177.06it/s]\n",
            "Avg similarity score vs gold = 0.480353\n",
            "Avg max score = 0.480353\n",
            "\n",
            " final normalized scores vs gold..\n",
            "\n",
            "Normalized pp score (acc_sim) = 0.3682 (300 / 404 valid)\n",
            "Normalized pp score (cola_sim) = 0.2764 (228 / 404 valid)\n",
            "Normalized pp score (acc_cola) = 0.4158 (168 / 404 valid)\n",
            "Normalized pp score (acc_cola_sim) = 0.2117 (168 / 404 valid)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JDwditFW6sW"
      },
      "source": [
        "We also convert the BPE-ed AAVE and SAE input of STRAP to text for our baseline comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLD29JNVXANO"
      },
      "source": [
        "# from transformers import GPT2Tokenizer\n",
        "# if not os.path.isdir(os.path.join(f'{REPO_PATH}', 'converted_texts')):\n",
        "#     os.mkdir(os.path.join(f'{REPO_PATH}', 'converted_texts'))\n",
        "# for s in ['aae', 'tweets']:\n",
        "#     if not os.path.isdir(os.path.join(f'{REPO_PATH}', 'converted_texts', s)):\n",
        "#         os.mkdir(os.path.join(f'{REPO_PATH}', 'converted_texts', s))\n",
        "\n",
        "#     tokenizer = GPT2Tokenizer.from_pretrained(f\"{REPO_PATH}/{s}\")\n",
        "#     bpe_fnames = list(filter(lambda x: x.split('.')[-1] == 'bpe', os.listdir(f\"{REPO_PATH}/cds/{s}\")))\n",
        "#     for fname in bpe_fnames:\n",
        "#         lines = list(map(lambda x: list(map(lambda y: int(y), x.split())), open(f\"{REPO_PATH}/cds/{s}/{fname}\").readlines()))\n",
        "#         texts = list(map(lambda line: tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(line)), lines))\n",
        "#         with open(f\"/content/drive/MyDrive/converted_texts/{s}/{fname.replace('bpe', 'txt')}\", 'w') as f:\n",
        "#             f.write('\\n'.join(texts) + '\\n')"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}