{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"STRAP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPrUTui5R7cCN5wmSiW8JY4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x9vTDpDGME0P","executionInfo":{"status":"ok","timestamp":1637729076136,"user_tz":300,"elapsed":141,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}},"outputId":"2ef13eaf-011a-4a2b-f1fb-0a7c3befa73f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","COLAB_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n","REPO_PATH = \"/content/drive/MyDrive/style-transfer-paraphrase\"\n","\n","%load_ext autoreload"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"tHHitYLuOby-"},"source":["Execute the readme setup from the [repo](https://github.com/martiansideofthemoon/style-transfer-paraphrase)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"efxUh_qdNQL7","executionInfo":{"status":"ok","timestamp":1637729101892,"user_tz":300,"elapsed":25055,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}},"outputId":"23eb88d2-ffba-4b0c-f84b-8776f36169ad"},"source":["# %cd \"/content/drive/MyDrive\"\n","# !git clone https://github.com/ktxlh/style-transfer-paraphrase.git\n","\n","%cd $REPO_PATH\n","\n","!pip install torch torchvision\n","!pip install -r requirements.txt\n","!pip install --editable .\n","\n","!cd fairseq\n","!pip install --editable .\n","\n","!pip install fairseq bitarray # workaround: to resolve some Colab specific module import error (https://github.com/pytorch/fairseq/issues/3093)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/style-transfer-paraphrase\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (8.0.0)\n","Collecting en-core-web-sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n","  Using cached en_core_web_sm-2.3.1-py3-none-any.whl\n","Requirement already satisfied: blessings==1.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.7)\n","Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.4.1)\n","Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.5.2)\n","Requirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.0.0)\n","Requirement already satisfied: certifi==2020.6.20 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2020.6.20)\n","Requirement already satisfied: cffi==1.14.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.14.3)\n","Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.0.4)\n","Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n","Requirement already satisfied: cymem==2.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.0.4)\n","Requirement already satisfied: Cython==0.29.21 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.29.21)\n","Requirement already satisfied: dataclasses==0.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.6)\n","Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (3.0.12)\n","Requirement already satisfied: flake8==3.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.8.4)\n","Requirement already satisfied: Flask==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.1.2)\n","Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (0.18.2)\n","Requirement already satisfied: gpustat==0.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (0.6.0)\n","Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.10)\n","Requirement already satisfied: importlib-metadata==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (2.0.0)\n","Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.1.0)\n","Requirement already satisfied: Jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (2.11.3)\n","Requirement already satisfied: joblib==0.17.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (0.17.0)\n","Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.1.1)\n","Requirement already satisfied: mccabe==0.6.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (0.6.1)\n","Requirement already satisfied: more-itertools==8.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (8.6.0)\n","Requirement already satisfied: murmurhash==1.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (1.0.4)\n","Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 27)) (3.5)\n","Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (1.19.2)\n","Requirement already satisfied: nvidia-ml-py3==7.352.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (7.352.0)\n","Requirement already satisfied: ordered-set==3.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 30)) (3.1.1)\n","Requirement already satisfied: ordered-set-stubs==0.1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 31)) (0.1.3)\n","Requirement already satisfied: packaging==20.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 32)) (20.4)\n","Requirement already satisfied: Paste==3.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 33)) (3.5.0)\n","Requirement already satisfied: pathtools==0.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 34)) (0.1.2)\n","Requirement already satisfied: Pillow==8.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (8.0.0)\n","Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 36)) (1.1.3)\n","Requirement already satisfied: poetry-version==0.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (0.1.5)\n","Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (2.0.0)\n","Requirement already satisfied: preshed==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 39)) (3.0.4)\n","Requirement already satisfied: profanity-filter==1.3.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 40)) (1.3.3)\n","Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 41)) (3.13.0)\n","Requirement already satisfied: psutil==5.7.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 42)) (5.7.3)\n","Requirement already satisfied: pycodestyle==2.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 43)) (2.6.0)\n","Requirement already satisfied: pycparser==2.20 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (2.20)\n","Requirement already satisfied: pydantic==1.7.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 45)) (1.7.4)\n","Requirement already satisfied: pyflakes==2.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 46)) (2.2.0)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 47)) (2.4.7)\n","Requirement already satisfied: redis==3.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 48)) (3.5.3)\n","Requirement already satisfied: regex==2020.10.15 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 49)) (2020.10.15)\n","Requirement already satisfied: requests==2.24.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 50)) (2.24.0)\n","Requirement already satisfied: ruamel.yaml==0.15.100 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 51)) (0.15.100)\n","Requirement already satisfied: sacrebleu==1.4.14 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 52)) (1.4.14)\n","Requirement already satisfied: sacremoses==0.0.43 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 53)) (0.0.43)\n","Requirement already satisfied: scipy==1.5.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 54)) (1.5.4)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 55)) (0.1.91)\n","Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 56)) (1.15.0)\n","Requirement already satisfied: spacy==2.3.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 57)) (2.3.2)\n","Requirement already satisfied: srsly==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 58)) (1.0.3)\n","Requirement already satisfied: tensorboardX==2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 59)) (2.1)\n","Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 60)) (7.4.1)\n","Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 61)) (0.9.2)\n","Requirement already satisfied: tomlkit==0.5.11 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 62)) (0.5.11)\n","Requirement already satisfied: tqdm==4.50.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 63)) (4.50.2)\n","Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 64)) (3.4.0)\n","Requirement already satisfied: urllib3==1.25.11 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 65)) (1.25.11)\n","Requirement already satisfied: waitress==1.4.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 66)) (1.4.4)\n","Requirement already satisfied: wasabi==0.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 67)) (0.8.0)\n","Requirement already satisfied: watchdog==0.10.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 68)) (0.10.3)\n","Requirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 69)) (1.0.1)\n","Requirement already satisfied: zipp==3.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 70)) (3.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from Paste==3.5.0->-r requirements.txt (line 33)) (57.4.0)\n","Obtaining file:///content/drive/My%20Drive/style-transfer-paraphrase\n","Installing collected packages: style-paraphrase\n","  Attempting uninstall: style-paraphrase\n","    Found existing installation: style-paraphrase 1.0\n","    Can't uninstall 'style-paraphrase'. No files were found to uninstall.\n","  Running setup.py develop for style-paraphrase\n","Successfully installed style-paraphrase-1.0\n","Obtaining file:///content/drive/My%20Drive/style-transfer-paraphrase\n","Installing collected packages: style-paraphrase\n","  Attempting uninstall: style-paraphrase\n","    Found existing installation: style-paraphrase 1.0\n","    Can't uninstall 'style-paraphrase'. No files were found to uninstall.\n","  Running setup.py develop for style-paraphrase\n","Successfully installed style-paraphrase-1.0\n","Requirement already satisfied: fairseq in /usr/local/lib/python3.7/dist-packages (0.10.2)\n","Requirement already satisfied: bitarray in /usr/local/lib/python3.7/dist-packages (2.3.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.10.0+cu111)\n","Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.4.14)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.50.2)\n","Requirement already satisfied: hydra-core in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.1.1)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.21)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2020.10.15)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.0.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.4.0)\n","Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (2.1.1)\n","Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (4.8)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (6.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core->fairseq) (3.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.10.0.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"qs0IkDORP9AB"},"source":["**`Restart runtime` now for the first time you run this notebook in each session**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLvSSNIlNQq3","executionInfo":{"status":"ok","timestamp":1637729119022,"user_tz":300,"elapsed":17148,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}},"outputId":"379f4f2e-85db-4cb0-b495-8c9ab5171cfe"},"source":["%cd /content/\n","!wget https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n","!tar -xzvf roberta.large.tar.gz\n","\n","# Add the following to your .bashrc file, feel free to store the model elsewhere on the hard disk\n","!export ROBERTA_LARGE=$PWD/roberta.large\n","%cd $REPO_PATH"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","--2021-11-24 04:45:01--  https://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 655283069 (625M) [application/gzip]\n","Saving to: ‘roberta.large.tar.gz.1’\n","\n","roberta.large.tar.g 100%[===================>] 624.93M  86.7MB/s    in 6.8s    \n","\n","2021-11-24 04:45:08 (92.1 MB/s) - ‘roberta.large.tar.gz.1’ saved [655283069/655283069]\n","\n","roberta.large/\n","roberta.large/dict.txt\n","roberta.large/model.pt\n","roberta.large/NOTE\n","/content/drive/MyDrive/style-transfer-paraphrase\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zswas7oTi26i"},"source":["custom data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-x3DWylfTA7","executionInfo":{"status":"ok","timestamp":1637729119168,"user_tz":300,"elapsed":166,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}},"outputId":"bbcf33ab-c318-44a2-f8bb-85f6ae1c6eca"},"source":["aave_data_name, sae_data_name = \"aae\", \"sae\"\n","!mkdir datasets/$aave_data_name datasets/$sae_data_name"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘datasets/aae’: File exists\n","mkdir: cannot create directory ‘datasets/sae’: File exists\n"]}]},{"cell_type":"code","metadata":{"id":"eTwh9C_SiyBq","executionInfo":{"status":"ok","timestamp":1637729119548,"user_tz":300,"elapsed":382,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}}},"source":["import os\n","import pandas as pd\n","data_input_path = f\"{COLAB_PATH}/AAVE_SAE/data/training\"\n","split_paths = {x.split('_')[0]: os.path.join(data_input_path, x) for x in os.listdir(data_input_path)}\n","\n","split_name_map = {\n","    'train': 'train',\n","    'val': 'dev',\n","    'test': 'test',\n","}\n","def conver_to_strap_data(split_name, split_path):\n","    df = pd.read_csv(split_path)\n","    aave_x = df.sentence[df.label==1].tolist()\n","    sae_x = df.sentence[df.label==0].tolist()\n","    with open(os.path.join('datasets', aave_data_name, split_name_map[split_name] + '.txt'), 'w') as f:\n","        f.write('\\n'.join(aave_x) + '\\n')\n","    with open(os.path.join('datasets', sae_data_name, split_name_map[split_name] + '.txt'), 'w') as f:\n","        f.write('\\n'.join(sae_x) + '\\n')\n","    with open(os.path.join('datasets', aave_data_name, split_name_map[split_name] + '.label'), 'w') as f:\n","        f.write(''.join([aave_data_name + '\\n' for _ in range(len(aave_x))]))\n","    with open(os.path.join('datasets', sae_data_name, split_name_map[split_name] + '.label'), 'w') as f:\n","        f.write(''.join([sae_data_name + '\\n' for _ in range(len(sae_x))]))\n","\n","for k, v in split_paths.items():\n","    conver_to_strap_data(k, v)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bB6ZBz0a3Vm-"},"source":["BPE conversion: steps from readme.md\n","\n","1. To convert a plaintext dataset into it's BPE form run the command,"]},{"cell_type":"code","metadata":{"id":"GjXON74o29vb","executionInfo":{"status":"ok","timestamp":1637726576281,"user_tz":300,"elapsed":128,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}}},"source":["!python datasets/dataset2bpe.py --dataset datasets/$aave_data_name\n","!python datasets/dataset2bpe.py --dataset datasets/$sae_data_name"],"execution_count":166,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tg3sYNL49lxV"},"source":["2. Next, for converting the BPE codes to fairseq binaries and building a label dictionary, first make sure you have downloaded RoBERTa and setup the $ROBERTA_LARGE global variable in your .bashrc (see \"Setup\" for more details). Then run,"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ow9MDjBh8c3r","executionInfo":{"status":"ok","timestamp":1637727045276,"user_tz":300,"elapsed":14208,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}},"outputId":"34aff18d-231a-461b-d898-6db6a9dfe457"},"source":["!chmod +x datasets/bpe2binary.sh\n","!cp \"/content/roberta.large/dict.txt\" /dict.txt\n","!datasets/bpe2binary.sh datasets/$aave_data_name\n","!datasets/bpe2binary.sh datasets/$sae_data_name"],"execution_count":170,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-11-24 04:10:33 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='datasets/aae-bin/label', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang=None, srcdict=None, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='datasets/aae/train.label', user_dir=None, validpref='datasets/aae/dev.label', workers=24)\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/fairseq-preprocess\", line 8, in <module>\n","    sys.exit(cli_main())\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 394, in cli_main\n","    main(args)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 74, in main\n","    raise FileExistsError(dict_path(args.source_lang))\n","FileExistsError: datasets/aae-bin/label/dict.txt\n","2021-11-24 04:10:35 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='datasets/aae-bin/input0', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang=None, srcdict='/dict.txt', target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='datasets/aae/train.input0.bpe', user_dir=None, validpref='datasets/aae/dev.input0.bpe', workers=24)\n","2021-11-24 04:10:35 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n","2021-11-24 04:10:36 | INFO | fairseq_cli.preprocess | [None] datasets/aae/train.input0.bpe: 1615 sents, 43711 tokens, 0.0% replaced by <unk>\n","2021-11-24 04:10:36 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n","2021-11-24 04:10:38 | INFO | fairseq_cli.preprocess | [None] datasets/aae/dev.input0.bpe: 202 sents, 5358 tokens, 0.0% replaced by <unk>\n","2021-11-24 04:10:38 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to datasets/aae-bin/input0\n","2021-11-24 04:10:40 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='datasets/sae-bin/label', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang=None, srcdict=None, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='datasets/sae/train.label', user_dir=None, validpref='datasets/sae/dev.label', workers=24)\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/fairseq-preprocess\", line 8, in <module>\n","    sys.exit(cli_main())\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 394, in cli_main\n","    main(args)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 74, in main\n","    raise FileExistsError(dict_path(args.source_lang))\n","FileExistsError: datasets/sae-bin/label/dict.txt\n","2021-11-24 04:10:41 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='datasets/sae-bin/input0', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang=None, srcdict='/dict.txt', target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='datasets/sae/train.input0.bpe', user_dir=None, validpref='datasets/sae/dev.input0.bpe', workers=24)\n","2021-11-24 04:10:42 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n","2021-11-24 04:10:43 | INFO | fairseq_cli.preprocess | [None] datasets/sae/train.input0.bpe: 1615 sents, 46383 tokens, 0.0% replaced by <unk>\n","2021-11-24 04:10:43 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n","2021-11-24 04:10:44 | INFO | fairseq_cli.preprocess | [None] datasets/sae/dev.input0.bpe: 202 sents, 5501 tokens, 0.0% replaced by <unk>\n","2021-11-24 04:10:44 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to datasets/sae-bin/input0\n"]}]},{"cell_type":"markdown","metadata":{"id":"KJLn40h3-2wl"},"source":["3. To train inverse paraphrasers you will need to paraphrase the dataset. First, download the pretrained model paraphraser_gpt2_large from here. After downloading the pretrained paraphrase model run the command,"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D8rNvxmW_Oi3","executionInfo":{"status":"ok","timestamp":1637727023428,"user_tz":300,"elapsed":407860,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}},"outputId":"ea8a33cc-1f5c-40e7-9a71-f1b3b1081833"},"source":["!python datasets/paraphrase_splits.py --dataset datasets/$aave_data_name\n","!python datasets/paraphrase_splits.py --dataset datasets/$sae_data_name"],"execution_count":168,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n","2021-11-24 04:03:40 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at /root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n","/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n","  message=\"hydra.experimental.initialize() is no longer experimental.\"\n","/usr/local/lib/python3.7/dist-packages/hydra/experimental/compose.py:19: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n","  message=\"hydra.experimental.compose() is no longer experimental.\"\n","/usr/local/lib/python3.7/dist-packages/hydra/core/default_element.py:126: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n","See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n","  See {url} for more information\"\"\"\n","/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:421: UserWarning: \n","'config' is validated against ConfigStore schema with the same name.\n","This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n","See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n","  state = load_checkpoint_to_cpu(filename, arg_overrides)\n","/usr/local/lib/python3.7/dist-packages/hydra/compose.py:54: UserWarning: \n","The strict flag in the compose API is deprecated and will be removed in the next version of Hydra.\n","See https://hydra.cc/docs/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n","\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n","  message=\"hydra.experimental.initialize() is no longer experimental.\"\n","/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:285: UserWarning: \n","'config' is validated against ConfigStore schema with the same name.\n","This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n","See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n","  **kwargs,\n","2021-11-24 04:03:41 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n","2021-11-24 04:03:44 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 512, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19812, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 999999, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 999999, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 512}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='loss', bpe='gpt2', bucket_cap_mb=200, clip_norm=0.0, cpu=False, criterion='masked_lm', curriculum=0, data='/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', dataset_impl='mmap', ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_no_spawn=False, distributed_port=19812, distributed_rank=0, distributed_world_size=512, dropout=0.1, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, find_unused_parameters=True, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, global_sync_iter=10, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, log_format='json', log_interval=25, lr=[0.0006], lr_scheduler='polynomial_decay', mask_prob=0.15, max_epoch=0, max_positions=512, max_sentences=16, max_sentences_valid=16, max_source_positions=512, max_target_positions=512, max_tokens=999999, max_update=500000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=2, only_validate=False, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, random_token_prob=0.1, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_interval=1, save_interval_updates=2000, seed=1, sentence_avg=False, skip_invalid_size_inputs_valid_test=True, spectral_norm_classification_head=False, stop_min_lr=-1, task='masked_lm', tbmf_wrapper=False, threshold_loss_scale=1.0, tokenizer=None, tokens_per_sample=512, total_num_update=500000, train_subset='train', untie_weights_roberta=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=24000, weight_decay=0.01), 'task': {'_name': 'masked_lm', 'data': '/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n","Loading paraphraser...\n","Some weights of the model checkpoint at paraphraser_gpt2_large were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","minibatches of train split done...:   0% 0/26 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n","minibatches of train split done...: 100% 26/26 [01:51<00:00,  4.27s/it]\n","minibatches of dev split done...: 100% 4/4 [00:14<00:00,  3.64s/it]\n","minibatches of test split done...: 100% 4/4 [00:14<00:00,  3.64s/it]\n","Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n","2021-11-24 04:07:06 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at /root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n","/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n","  message=\"hydra.experimental.initialize() is no longer experimental.\"\n","/usr/local/lib/python3.7/dist-packages/hydra/experimental/compose.py:19: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n","  message=\"hydra.experimental.compose() is no longer experimental.\"\n","/usr/local/lib/python3.7/dist-packages/hydra/core/default_element.py:126: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n","See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n","  See {url} for more information\"\"\"\n","/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:421: UserWarning: \n","'config' is validated against ConfigStore schema with the same name.\n","This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n","See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n","  state = load_checkpoint_to_cpu(filename, arg_overrides)\n","/usr/local/lib/python3.7/dist-packages/hydra/compose.py:54: UserWarning: \n","The strict flag in the compose API is deprecated and will be removed in the next version of Hydra.\n","See https://hydra.cc/docs/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n","\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n","  message=\"hydra.experimental.initialize() is no longer experimental.\"\n","/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:285: UserWarning: \n","'config' is validated against ConfigStore schema with the same name.\n","This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n","See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n","  **kwargs,\n","2021-11-24 04:07:08 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n","2021-11-24 04:07:11 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 512, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19812, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 999999, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 999999, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 512}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='loss', bpe='gpt2', bucket_cap_mb=200, clip_norm=0.0, cpu=False, criterion='masked_lm', curriculum=0, data='/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', dataset_impl='mmap', ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_no_spawn=False, distributed_port=19812, distributed_rank=0, distributed_world_size=512, dropout=0.1, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, find_unused_parameters=True, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, global_sync_iter=10, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, log_format='json', log_interval=25, lr=[0.0006], lr_scheduler='polynomial_decay', mask_prob=0.15, max_epoch=0, max_positions=512, max_sentences=16, max_sentences_valid=16, max_source_positions=512, max_target_positions=512, max_tokens=999999, max_update=500000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=2, only_validate=False, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, random_token_prob=0.1, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_interval=1, save_interval_updates=2000, seed=1, sentence_avg=False, skip_invalid_size_inputs_valid_test=True, spectral_norm_classification_head=False, stop_min_lr=-1, task='masked_lm', tbmf_wrapper=False, threshold_loss_scale=1.0, tokenizer=None, tokens_per_sample=512, total_num_update=500000, train_subset='train', untie_weights_roberta=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=24000, weight_decay=0.01), 'task': {'_name': 'masked_lm', 'data': '/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n","Loading paraphraser...\n","Some weights of the model checkpoint at paraphraser_gpt2_large were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","minibatches of train split done...:   0% 0/26 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n","minibatches of train split done...: 100% 26/26 [01:51<00:00,  4.27s/it]\n","minibatches of dev split done...: 100% 4/4 [00:14<00:00,  3.63s/it]\n","minibatches of test split done...: 100% 4/4 [00:14<00:00,  3.63s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"JYDGyb8ZDq70"},"source":["Step 4 and 5 are for training. We don't need them.\n","> 4. Add an entry to the DATASET_CONFIG dictionary in style_paraphrase/dataset_config.py, customizing configuration if needed.\n","> 5. Enter your dataset in the hyperparameters file (style_paraphrase/hyperparameters_config.py) and run python style_paraphrase/schedule.py.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yI1oKxNnHTKu","executionInfo":{"status":"ok","timestamp":1637729633079,"user_tz":300,"elapsed":449023,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}},"outputId":"fdf1cdf5-c3f6-49d3-f33c-0ace81e76513"},"source":["!chmod +x \"$REPO_PATH/style_paraphrase/evaluation/scripts/evaluate_twitter.sh\"\n","!style_paraphrase/evaluation/scripts/evaluate_twitter.sh \\\n","    \"$REPO_PATH/aae\" \"$REPO_PATH/tweets\" \"$REPO_PATH/paraphraser_gpt2_large\""],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","-------------------------------------------\n","Mode nucleus_paraphrase  --- top-p 0.0, split test\n","-------------------------------------------\n","\n","\n","translate sae to aae\n","datasets/sae/test.txt.paraphrase\n","Some weights of the model checkpoint at /content/drive/MyDrive/style-transfer-paraphrase/aae were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","transferring dataset...:   0% 0/7 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n","transferring dataset...: 100% 7/7 [00:16<00:00,  2.33s/it]\n","\n","translate aae to sae\n","datasets/aae/test.txt.paraphrase\n","Some weights of the model checkpoint at /content/drive/MyDrive/style-transfer-paraphrase/tweets were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","transferring dataset...:   0% 0/7 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n","transferring dataset...: 100% 7/7 [00:16<00:00,  2.32s/it]\n","Traceback (most recent call last):\n","  File \"style_paraphrase/evaluation/scripts/flip_labels.py\", line 3, in <module>\n","    from style_paraphrase.evaluation.similarity.test_sim import find_similarity\n","  File \"/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/evaluation/similarity/test_sim.py\", line 9, in <module>\n","    model = torch.load('style_paraphrase/evaluation/similarity/sim/sim.pt')\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 594, in load\n","    with _open_file_like(f, 'rb') as opened_file:\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n","    return _open_file(name_or_buffer, mode)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 211, in __init__\n","    super(_open_file, self).__init__(open(name, mode))\n","FileNotFoundError: [Errno 2] No such file or directory: 'style_paraphrase/evaluation/similarity/sim/sim.pt'\n","\n","-------------------------------------------\n","Mode nucleus_paraphrase  --- top-p 0.6, split test\n","-------------------------------------------\n","\n","\n","translate sae to aae\n","datasets/sae/test.txt.paraphrase\n","Some weights of the model checkpoint at /content/drive/MyDrive/style-transfer-paraphrase/aae were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","transferring dataset...:   0% 0/7 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n","transferring dataset...: 100% 7/7 [00:18<00:00,  2.62s/it]\n","\n","translate aae to sae\n","datasets/aae/test.txt.paraphrase\n","Some weights of the model checkpoint at /content/drive/MyDrive/style-transfer-paraphrase/tweets were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","transferring dataset...:   0% 0/7 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n","transferring dataset...: 100% 7/7 [00:18<00:00,  2.61s/it]\n","Traceback (most recent call last):\n","  File \"style_paraphrase/evaluation/scripts/flip_labels.py\", line 3, in <module>\n","    from style_paraphrase.evaluation.similarity.test_sim import find_similarity\n","  File \"/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/evaluation/similarity/test_sim.py\", line 9, in <module>\n","    model = torch.load('style_paraphrase/evaluation/similarity/sim/sim.pt')\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 594, in load\n","    with _open_file_like(f, 'rb') as opened_file:\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n","    return _open_file(name_or_buffer, mode)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 211, in __init__\n","    super(_open_file, self).__init__(open(name, mode))\n","FileNotFoundError: [Errno 2] No such file or directory: 'style_paraphrase/evaluation/similarity/sim/sim.pt'\n","\n","-------------------------------------------\n","Mode nucleus_paraphrase  --- top-p 0.9, split test\n","-------------------------------------------\n","\n","\n","translate sae to aae\n","datasets/sae/test.txt.paraphrase\n","Some weights of the model checkpoint at /content/drive/MyDrive/style-transfer-paraphrase/aae were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","transferring dataset...:   0% 0/7 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n","transferring dataset...: 100% 7/7 [00:18<00:00,  2.60s/it]\n","\n","translate aae to sae\n","datasets/aae/test.txt.paraphrase\n","Some weights of the model checkpoint at /content/drive/MyDrive/style-transfer-paraphrase/tweets were not used when initializing GPT2LMHeadModel: ['transformer.extra_embedding_project.weight', 'transformer.extra_embedding_project.bias']\n","- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","transferring dataset...:   0% 0/7 [00:00<?, ?it/s]/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/inference_utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  gpt2_sentences=torch.tensor([inst.sentence for inst in instances]).to(args.device),\n","transferring dataset...: 100% 7/7 [00:18<00:00,  2.62s/it]\n","Traceback (most recent call last):\n","  File \"style_paraphrase/evaluation/scripts/flip_labels.py\", line 3, in <module>\n","    from style_paraphrase.evaluation.similarity.test_sim import find_similarity\n","  File \"/content/drive/My Drive/style-transfer-paraphrase/style_paraphrase/evaluation/similarity/test_sim.py\", line 9, in <module>\n","    model = torch.load('style_paraphrase/evaluation/similarity/sim/sim.pt')\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 594, in load\n","    with _open_file_like(f, 'rb') as opened_file:\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n","    return _open_file(name_or_buffer, mode)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 211, in __init__\n","    super(_open_file, self).__init__(open(name, mode))\n","FileNotFoundError: [Errno 2] No such file or directory: 'style_paraphrase/evaluation/similarity/sim/sim.pt'\n"]}]},{"cell_type":"markdown","metadata":{"id":"7JDwditFW6sW"},"source":["順便把bpe轉成text"]},{"cell_type":"code","metadata":{"id":"WLD29JNVXANO","executionInfo":{"status":"ok","timestamp":1637732691806,"user_tz":300,"elapsed":5869,"user":{"displayName":"Shang-Ling Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtys7lFIqQtQ_Ye4WScOtb3-VCxl4p7NtMHIpzdRg=s64","userId":"17849572641192248527"}}},"source":["from transformers import GPT2Tokenizer\n","if not os.path.isdir(os.path.join(f'{REPO_PATH}', 'converted_texts')):\n","    os.mkdir(os.path.join(f'{REPO_PATH}', 'converted_texts'))\n","for s1, s2 in [('aae', 'aae'), ('tweets', 'sae')]:\n","    if not os.path.isdir(os.path.join(f'{REPO_PATH}', 'converted_texts', s2)):\n","        os.mkdir(os.path.join(f'{REPO_PATH}', 'converted_texts', s2))\n","\n","    tokenizer = GPT2Tokenizer.from_pretrained(f\"{REPO_PATH}/{s1}\")\n","    bpe_fnames = list(filter(lambda x: x.split('.')[-1] == 'bpe', os.listdir(f\"{REPO_PATH}/cds/{s1}\")))\n","    for fname in bpe_fnames:\n","        lines = list(map(lambda x: list(map(lambda y: int(y), x.split())), open(f\"{REPO_PATH}/cds/{s1}/{fname}\").readlines()))\n","        texts = list(map(lambda line: tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(line)), lines))\n","        with open(f\"{REPO_PATH}/converted_texts/{s2}/{fname.replace('bpe', 'txt')}\", 'w') as f:\n","            f.write('\\n'.join(texts) + '\\n')"],"execution_count":49,"outputs":[]}]}